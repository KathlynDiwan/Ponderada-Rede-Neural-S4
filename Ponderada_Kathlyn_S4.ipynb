{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importação das bibliotecas"
      ],
      "metadata": {
        "id": "UwM6lWAlHNB5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SZQ14AiBe-F",
        "outputId": "a098c733-cea7-4ed6-edea-8a45737c5a81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import dask.dataframe as dd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization, Conv1D, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dens\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import accuracy_score, recall_score, f1_score, roc_auc_score\n",
        "from tensorflow.keras.optimizers import Adam\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carregamento do aqruivo: cartão de crédito"
      ],
      "metadata": {
        "id": "ntk-4OEZHRMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar os dados\n",
        "data = pd.read_csv('/content/drive/MyDrive/ponderada/creditcard (1).csv')\n",
        "\n"
      ],
      "metadata": {
        "id": "L-ottkqHBiQN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Divisão conjunto treino e teste / normalização"
      ],
      "metadata": {
        "id": "1pCZT_44HZAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.drop('Class', axis=1)\n",
        "y = data['Class']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "Hw-nZyFuHYRT"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo 1 - o meu modelo original em que estarei utilizando o Keras"
      ],
      "metadata": {
        "id": "L9Jn9QcxHfn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Dense(64, input_dim=X_train.shape[1], activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Dense(32, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=64,\n",
        "                    validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Egaqsdl3CKru",
        "outputId": "b4984c2f-624d-4953-fe3c-a9b5c2fb97df"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m3116/3116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - accuracy: 0.9126 - loss: 0.2101 - val_accuracy: 0.9994 - val_loss: 0.0070\n",
            "Epoch 2/10\n",
            "\u001b[1m3116/3116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - accuracy: 0.9990 - loss: 0.0063 - val_accuracy: 0.9994 - val_loss: 0.0076\n",
            "Epoch 3/10\n",
            "\u001b[1m3116/3116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - accuracy: 0.9990 - loss: 0.0058 - val_accuracy: 0.9994 - val_loss: 0.0068\n",
            "Epoch 4/10\n",
            "\u001b[1m3116/3116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 4ms/step - accuracy: 0.9991 - loss: 0.0050 - val_accuracy: 0.9994 - val_loss: 0.0057\n",
            "Epoch 5/10\n",
            "\u001b[1m3116/3116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.9992 - loss: 0.0043 - val_accuracy: 0.9994 - val_loss: 0.0064\n",
            "Epoch 6/10\n",
            "\u001b[1m3116/3116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.9992 - loss: 0.0038 - val_accuracy: 0.9994 - val_loss: 0.0068\n",
            "Epoch 7/10\n",
            "\u001b[1m3116/3116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.9992 - loss: 0.0040 - val_accuracy: 0.9994 - val_loss: 0.0067\n",
            "Epoch 8/10\n",
            "\u001b[1m3116/3116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - accuracy: 0.9993 - loss: 0.0039 - val_accuracy: 0.9994 - val_loss: 0.0071\n",
            "Epoch 9/10\n",
            "\u001b[1m3116/3116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.9992 - loss: 0.0039 - val_accuracy: 0.9994 - val_loss: 0.0061\n",
            "Epoch 10/10\n",
            "\u001b[1m3116/3116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - accuracy: 0.9992 - loss: 0.0039 - val_accuracy: 0.9994 - val_loss: 0.0057\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo 2 - Arquitetura com diferentes camadas e quantidades de neurônios (Batch).\n"
      ],
      "metadata": {
        "id": "pkT5lQIdHpFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_batchnorm = Sequential([\n",
        "    Dense(128, input_dim=X_train.shape[1]),\n",
        "    BatchNormalization(),\n",
        "    tf.keras.layers.Activation('relu'),\n",
        "    Dropout(0.5),  # Adicionando Dropout para evitar overfitting\n",
        "    Dense(64),\n",
        "    BatchNormalization(),\n",
        "    tf.keras.layers.Activation('relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')  # Como é um problema binário, usamos sigmoid\n",
        "])\n",
        "\n",
        "model_batchnorm.compile(optimizer='adam',\n",
        "                        loss='binary_crossentropy',\n",
        "                        metrics=['accuracy'])\n",
        "\n",
        "history_batchnorm = model_batchnorm.fit(X_train, y_train,\n",
        "                                        epochs=10,\n",
        "                                        batch_size=64,\n",
        "                                        validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oL2Sb_rEDUdu",
        "outputId": "a2d9a8f3-7137-40a3-8615-5a4386f05d9c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m3116/3116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 4ms/step - accuracy: 0.9773 - loss: 0.0719 - val_accuracy: 0.9994 - val_loss: 0.0041\n",
            "Epoch 2/10\n",
            "\u001b[1m3116/3116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.9992 - loss: 0.0048 - val_accuracy: 0.9994 - val_loss: 0.0043\n",
            "Epoch 3/10\n",
            "\u001b[1m3116/3116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.9992 - loss: 0.0048 - val_accuracy: 0.9994 - val_loss: 0.0038\n",
            "Epoch 4/10\n",
            "\u001b[1m3116/3116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.9993 - loss: 0.0040 - val_accuracy: 0.9994 - val_loss: 0.0040\n",
            "Epoch 5/10\n",
            "\u001b[1m3116/3116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.9992 - loss: 0.0039 - val_accuracy: 0.9994 - val_loss: 0.0049\n",
            "Epoch 6/10\n",
            "\u001b[1m3116/3116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.9993 - loss: 0.0038 - val_accuracy: 0.9994 - val_loss: 0.0041\n",
            "Epoch 7/10\n",
            "\u001b[1m3116/3116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - accuracy: 0.9993 - loss: 0.0043 - val_accuracy: 0.9994 - val_loss: 0.0035\n",
            "Epoch 8/10\n",
            "\u001b[1m3116/3116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.9992 - loss: 0.0040 - val_accuracy: 0.9994 - val_loss: 0.0035\n",
            "Epoch 9/10\n",
            "\u001b[1m3116/3116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.9994 - loss: 0.0036 - val_accuracy: 0.9994 - val_loss: 0.0036\n",
            "Epoch 10/10\n",
            "\u001b[1m3116/3116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.9993 - loss: 0.0034 - val_accuracy: 0.9994 - val_loss: 0.0036\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculando a Acurácia"
      ],
      "metadata": {
        "id": "DIBsSWEYH4y9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss1, test_accuracy1 = model.evaluate(X_test, y_test)\n",
        "test_loss2, test_accuracy2 = model_batchnorm.evaluate(X_test, y_test)\n",
        "\n",
        "print(f\"Acurácia do Modelo 1: {test_accuracy1}\")\n",
        "print(f\"Acurácia do Modelo 2: {test_accuracy2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5h4hy3UEao5",
        "outputId": "38042236-524e-42a9-a539-13bf28d3b67a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2671/2671\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.9993 - loss: 0.0065\n",
            "\u001b[1m2671/2671\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9993 - loss: 0.0042\n",
            "Acurácia do Modelo 1: 0.9993680119514465\n",
            "Acurácia do Modelo 2: 0.9993680119514465\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cálculo do AUC-ROC"
      ],
      "metadata": {
        "id": "y3eR1_ZSHFgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred1 = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "y_pred2 = (model_batchnorm.predict(X_test) > 0.5).astype(\"int32\")\n",
        "\n",
        "# Calculando AUC-ROC para ambos os modelos\n",
        "auc_roc1 = roc_auc_score(y_test, y_pred1)\n",
        "auc_roc2 = roc_auc_score(y_test, y_pred2)\n",
        "\n",
        "print(f\"AUC-ROC do Modelo 1: {auc_roc1}\")\n",
        "print(f\"AUC-ROC do Modelo 2: {auc_roc2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9L2tiL2Ee4e",
        "outputId": "d8592c27-e889-42d3-f7db-36cde10643a4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2671/2671\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
            "\u001b[1m2671/2671\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
            "AUC-ROC do Modelo 1: 0.9189300891796344\n",
            "AUC-ROC do Modelo 2: 0.9152594797751237\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Criação do relatório para compararmos ambos modelos"
      ],
      "metadata": {
        "id": "Oiup5FgMH75W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Modelo 1\")\n",
        "print(classification_report(y_test, y_pred1))\n",
        "\n",
        "print(\"Modelo 2 \")\n",
        "print(classification_report(y_test, y_pred2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JlXgDHZEqS1",
        "outputId": "65f5e9a4-564c-4f08-9c62-920efade55e1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo 1\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     85307\n",
            "           1       0.78      0.84      0.81       136\n",
            "\n",
            "    accuracy                           1.00     85443\n",
            "   macro avg       0.89      0.92      0.90     85443\n",
            "weighted avg       1.00      1.00      1.00     85443\n",
            "\n",
            "Modelo 2 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     85307\n",
            "           1       0.78      0.83      0.81       136\n",
            "\n",
            "    accuracy                           1.00     85443\n",
            "   macro avg       0.89      0.92      0.90     85443\n",
            "weighted avg       1.00      1.00      1.00     85443\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Análises e conclusões"
      ],
      "metadata": {
        "id": "RdKoQGTwE-da"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Acurácia de treinamento:** A acurácia dos modelos durante o treinamento e validação parece muito alta, próxima de 100%. ALgo bem comum e esperado nesses problemas de detecção de fraudes em cartões de crédito. No entanto, uma acurácia alta não necessariamente indica um bom desempenho na detecção de fraudes, que é a classe minoritária e a mais relevante neste contexto.\n",
        "\n",
        "Outro fator de grande relevância a ser analisado é **AUC-ROC** O AUC-ROC que é uma métrica importante para avaliar a capacidade do modelo em diferenciar entre as classes  (fraude) e (não fraude). Sabemos que essa métrica varia entre 0-1, e em ambos os modelos os valores de AUC_ROC foram bem próximos. O valor do modelo 1 (0.9189) foi maior do que o valor encontrado no modelo 2(0.9153) que indica que ele tem uma capacidade ligeiramente melhor de distinguir entre fraudes e não fraudes em comparação com o Modelo 2. Porém vemos que a diferença entre os dois valores é pequena, sugerindo que ambos os modelos têm desempenho  similar.\n",
        "\n",
        "Os **relatório de classificação** fornecem uma visão de três métricas principais: precision, recall e f1-score.\n",
        "\n",
        "A precisão para a classe de fraude (classe 1) é de 0.78 para ambos os modelos, o que mostra que o modelo prevê uma fraude, ele está correto 78% das vezes.\n",
        "\n",
        "O recall é maior no Modelo 1 (0.84) em comparação ao Modelo 2 (0.83). Esse valor indica que o Modelo 1 consegue identificar uma maior quantidade das fraudes presentes no conjunto de dados.\n",
        "\n",
        "Ambos os modelos possuem um f1-score de 0.81, indicando que o equilíbrio entre precisão e recall é semelhante em ambos os casos.\n",
        "\n",
        "**Conclusão**:\n",
        "Ambos os modelos têm desempenho muito semelhante em termos de precisão, recall, f1-score e AUC-ROC. No entanto, o Modelo 1 se destaca com um AUC-ROC um pouco maior. Dessa forma, o modelo 1 pode ser preferido por apresentar valores maiores em algumas métricas o que o torna um pouco mais eficaz na detecção de fraudes sem comprometer muito a precisão. O modelo 2, apesar de ter um AUC-ROCinferior ainda apresenta um desempenho bom e também poderia ser escolhido.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "h00OI5dXFCw_"
      }
    }
  ]
}